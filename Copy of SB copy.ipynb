{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPOBa5CzZqncg0MJoJoJgjC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zNRx9_ozHrUN"},"outputs":[],"source":["\n","# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n","# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'mental-health-corpus:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2782228%2F4805127%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240403%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240403T120552Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Df3946dd3cff7845f439aee3e4ab5265e645d59539b39c3fa5613644dc7d21f149635bc320a163c3d09211a0de2c56534ca91e63de1c43dc8a19f133354bce319e96305a4925365aee1bf9430c80c745b8119840ca4bfdab7cde37f23426a54af8628efd9ee8b4c8a0fd73ba26f29773610f151c4a6848968ddb9c82e3923f3375dd9dcfb9ad9e3c9985b5cbc9175130708bd4b5d2f80aad99d721dd7d72517e43772833332208e4e91e765015e06aa661ce3c1a577a63c2bfa10b1b0f97eb3ca2379f630fb1e8079876ccb05da594cf07c4fc2739d6ca34d5cad41da9e5e5633b83d341766bfb8c66413d65f3c248935adeedcdf8d261b6f453c659e387344d5'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')\n"]},{"cell_type":"code","source":["pip install unidecode"],"metadata":{"id":"0v86HA3FH0KM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kz_dM4RHyulX"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","from unidecode import unidecode\n","\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import collections\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import backend as K\n","from tensorflow.keras import layers\n","from tensorflow.keras.optimizers import Adam\n","\n","from google.colab import files\n","import copy\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"code","source":["def cleanData(df, textLabel = 'text'):\n","    # Variavel para substituir todos os caracteres que não são letras ou espaços em branco\n","    regex = re.compile('[^a-z\\s]')\n","    # Transforma o texto em minusculo\n","    df['clean_text'] = df[textLabel].str.lower()\n","    # Remove todos os caracteres que não são letras ou espaços\n","    df['clean_text'] = df['clean_text'].apply(lambda x: regex.sub('', x))\n","    # Remove palavras com menos de 3 caracteres\n","    df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 2]))\n","\n","    # Tokenization\n","    df['clean_text'] = df['clean_text'].apply(word_tokenize)\n","\n","    # Removing stopwords\n","    stop_words = set(stopwords.words('english'))\n","    df['clean_text'] = df['clean_text'].apply(lambda x: [word for word in x if word not in stop_words])\n","\n","    # Word normalization\n","    stemmer = SnowballStemmer('english')\n","    df['clean_text'] = df['clean_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n","\n","    # Joining the words back into a single text\n","    df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join(x))\n","\n","    return df"],"metadata":{"id":"xHMvEyO4JJ_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def PLOTFun(top_words):\n","    # Figure Size and set background\n","    plt.figure(figsize = (10, 6))\n","    plt.style.use('dark_background')\n","\n","    # Create the Barplot\n","    plt.bar(range(len(top_words)), list(top_words.values()), align = 'center')\n","\n","    # Creating a y axis with words\n","    plt.xticks(range(len(top_words)), list(top_words.keys()))\n","\n","    # Grid Opacity\n","    plt.grid(alpha = 0.5)\n","    # Title and labels\n","    plt.title('Top 10 most used words', fontsize = 18)\n","    plt.xlabel('Words')\n","    plt.ylabel('Frequency')"],"metadata":{"id":"EhurTxSNJMnq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDRSsnqtzkUX"},"outputs":[],"source":["def DataLabelFormation(df, max_words = 10000, max_len = 200, textLabel = 'text', labelLABEL = 'label'):\n","    # Maximum number of words to be considered in the vocabulary\n","    max_words = max_words\n","    # Maximum number of tokens in a sequence\n","    max_len = max_len\n","    # Tokenizer\n","    tokenizer = Tokenizer(num_words = max_words)\n","    # Snap tokenizer to text data\n","    tokenizer.fit_on_texts(df[textLabel])\n","    # Converts texts into strings of numbers\n","    sequences = tokenizer.texts_to_sequences(df[textLabel])\n","    # Mapping words to indexes\n","    word_index = tokenizer.word_index\n","\n","    # Sequence padding\n","    data = pad_sequences(sequences, maxlen = max_len)\n","\n","    # Converting labels to numeric format\n","    labels = tf.keras.utils.to_categorical(df[labelLABEL])\n","\n","    return data, labels\n"]},{"cell_type":"code","source":["from keras.optimizers import SGD\n","custom_learning_rate = 0.001\n","optimizer = SGD(learning_rate=custom_learning_rate)"],"metadata":{"id":"Z06CL1spJ0tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import backend as K"],"metadata":{"id":"Rx9kPmzyJ1gq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.saving import register_keras_serializable"],"metadata":{"id":"zfeHoBhSacXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@register_keras_serializable()\n","def specificity(y_true, y_pred):\n","    true_negatives = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.equal(y_true, 0), tf.equal(tf.round(y_pred), 0)), dtype=tf.float32))\n","    possible_negatives = tf.reduce_sum(tf.cast(tf.equal(y_true, 0), dtype=tf.float32))\n","    return true_negatives / (possible_negatives + tf.keras.backend.epsilon())\n","@register_keras_serializable()\n","def sensitivity(y_true, y_pred):\n","    true_positives = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.equal(y_true, 1), tf.equal(tf.round(y_pred), 1)), dtype=tf.float32))\n","    possible_positives = tf.reduce_sum(tf.cast(tf.equal(y_true, 1), dtype=tf.float32))\n","    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n","@register_keras_serializable()\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","@register_keras_serializable()\n","def precisionM(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","@register_keras_serializable()\n","def f1_S(y_true, y_pred):\n","    precision = precisionM(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"metadata":{"id":"70Ty70mfJ6-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0X5d3SdsztMf"},"outputs":[],"source":["def buildModel(max_words, max_len, embedLayerNum = 128, LSTMNum = 64, reluNum = 32, softmaxNum = 2, dropout = 0.3, recurrent_dropout=0.2, loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy',sensitivity, specificity,f1_S]):\n","    # Model\n","    model = Sequential()\n","    model.add(Embedding(max_words, embedLayerNum, input_length = max_len))\n","    model.add(Bidirectional(LSTM(LSTMNum, dropout = dropout, recurrent_dropout = recurrent_dropout)))\n","    model.add(Dense(reluNum, activation = 'relu'))\n","    model.add(Dense(softmaxNum, activation = 'softmax'))\n","\n","    # Compile the model\n","    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n","\n","    return model"]},{"cell_type":"code","source":["def printWeight(model):\n","    for layer in model.layers:\n","        if len(layer.weights) > 1:\n","            weights = layer.get_weights()[0]\n","            biases = layer.get_weights()[1]\n","            print(f\"Layer {layer.name} weights shape: {weights.shape}\")\n","            print(weights)\n","        else:\n","            weights = layer.get_weights()[0]\n","            print(f\"Layer {layer.name} weights shape: {weights.shape}\")\n","            print(weights)"],"metadata":{"id":"DByR6n-8oi-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nSXsmOuwzB5I"},"outputs":[],"source":["# read data and check labels count\n","df = pd.read_csv('/kaggle/input/mental-health-corpus/mental_health.csv')\n","\n","df['label'].value_counts()"]},{"cell_type":"code","source":["# clean data and form words matrix\n","\n","df = cleanData(df)\n","words = []\n","for text in df['clean_text']:\n","    words.extend(text.split())\n","word_count = collections.Counter(words)\n","top_words = dict(word_count.most_common(10))"],"metadata":{"id":"RSuRjI5WLx_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot to visulize word frequencies\n","PLOTFun(top_words)"],"metadata":{"id":"tc8GXV7WL9cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# labelling data and tokenization/word-2-Vec formation\n","max_words = 10000\n","max_len = 200\n","data, labels = DataLabelFormation(df, max_words, max_len)"],"metadata":{"id":"XCwNLYJUMTVu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = buildModel(max_words, max_len, embedLayerNum = 128, LSTMNum = 64, reluNum = 32, softmaxNum = 2, dropout = 0.3, recurrent_dropout=0.2, loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy',sensitivity, specificity,f1_S])"],"metadata":{"id":"jX9kC3VIMlJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(data, labels, validation_split = 0.2, epochs = 5, batch_size = 32)"],"metadata":{"id":"72fSmnfgMspv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import copy"],"metadata":{"id":"ZL0cmqFULyt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["backupmodel = copy.deepcopy(model)"],"metadata":{"id":"nDAw--OWLxVa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = copy.deepcopy(backupmodel)"],"metadata":{"id":"1Phaq4xoL0Ur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["uploaded = files.upload()"],"metadata":{"id":"ebRLkt_rW5MN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfOurs.head()"],"metadata":{"id":"nEMk69z2XtqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfOurs = pd.read_csv('Survey.csv')\n","\n","LABEL = 'Label'\n","TEXT = 'Text'"],"metadata":{"id":"amRou63qXAEm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfOurs[LABEL] = dfOurs[LABEL].fillna(0)"],"metadata":{"id":"MnxEIKeVXH4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72q2xb4USoFz"},"outputs":[],"source":["dfOurs = cleanData(dfOurs, textLabel=TEXT)\n","\n","wordsOurs = []\n","for text in dfOurs['clean_text']:\n","    wordsOurs.extend(text.split())\n","word_count_ours = collections.Counter(wordsOurs)\n","top_words_ours = dict(word_count_ours.most_common(10))\n","\n","PLOTFun(top_words_ours)"]},{"cell_type":"code","source":["# Maximum number of words to be considered in the vocabulary\n","max_words_Ours = 200\n","# Maximum number of tokens in a sequence\n","max_len_ours = 200\n","# Tokenizer\n","dataOurs, labelsOurs = DataLabelFormation(dfOurs, max_words_Ours, max_len_ours, textLabel = TEXT, labelLABEL = LABEL)"],"metadata":{"id":"W54p1rJrYxtv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(labelsOurs)"],"metadata":{"id":"PRO-1fSiJocX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(dataOurs, labelsOurs, validation_split = 0.3, epochs = 5, batch_size = 8)"],"metadata":{"id":"NylvZ0c9Y8eA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(labelsOurs)"],"metadata":{"id":"QAj38OxcJ221"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ACCURACY GRAPH\n","plt.style.use('seaborn')\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"PghCNHbiZXog"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Specificity\n","plt.style.use('seaborn')\n","plt.plot(history.history['specificity'], label='Training Specificity')\n","plt.plot(history.history['val_specificity'], label='Validation Specificity')\n","plt.title('Model Specificity')\n","plt.xlabel('Epoch')\n","plt.ylabel('Specificity')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"HqNqAGjbZbHn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Sensitivity\n","plt.style.use('seaborn')\n","plt.plot(history.history['sensitivity'], label='Training Sensitivity')\n","plt.plot(history.history['val_sensitivity'], label='Validation Sensitivity')\n","plt.title('Model Sensitivity')\n","plt.xlabel('Epoch')\n","plt.ylabel('Sensitivity')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"5NCSiTwYZfin"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#F1_Score\n","plt.style.use('seaborn')\n","plt.plot(history.history['f1_S'], label='Training f1_S')\n","plt.plot(history.history['val_f1_S'], label='Validation f1_S')\n","plt.title('Model F1_Score')\n","plt.xlabel('Epoch')\n","plt.ylabel('f1_S')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"DqX-MvKrZi2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Loss\n","plt.style.use('seaborn')\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc = 'upper right')\n","plt.show()"],"metadata":{"id":"57QR6MrnZlj3"},"execution_count":null,"outputs":[]}]}