{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AlfEv3O5HsLk"},"outputs":[],"source":["pip install unidecode"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","from unidecode import unidecode\n","\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","import collections\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"-913s-mtHzE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"id":"MYP2mIPjH6UY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('Survey.csv')"],"metadata":{"id":"wwkHX8sAICdY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Text'] = df['Text'].astype(str)"],"metadata":{"id":"OPvlnfhkyUD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Label'] = df['Label'].fillna(0)"],"metadata":{"id":"NhxZS4KwIFzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Label'] = df['Label'].astype(int)"],"metadata":{"id":"DARHWMyGK3eg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nnT1F3C-x2Ns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"UyAXS7skIKbB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re"],"metadata":{"id":"I_WSF48-MFVH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Pre-Processing"],"metadata":{"id":"UqgfpuAyIODh"}},{"cell_type":"code","source":["# Variavel para substituir todos os caracteres que não são letras ou espaços em branco\n","regex = re.compile('[^a-z\\s]')\n","# Transforma o texto em minusculo\n","df['clean_text'] = df['Text'].str.lower()\n","\n","# Remove todos os caracteres que não são letras ou espaços\n","df['clean_text'] = df['clean_text'].apply(lambda x: regex.sub('', x))\n","# Remove palavras com menos de 3 caracteres\n","df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 2]))"],"metadata":{"id":"O3fwjYUTYgrg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df['clean_text'])"],"metadata":{"id":"bQe0Ihe7GPms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"id":"XL2wPL_gIUf5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenization\n","df['clean_text'] = df['clean_text'].apply(word_tokenize)\n","\n","# Removing stopwords\n","stop_words = set(stopwords.words('english'))\n","df['clean_text'] = df['clean_text'].apply(lambda x: [word for word in x if word not in stop_words])\n","\n","# Word normalization\n","stemmer = SnowballStemmer('english')\n","df['clean_text'] = df['clean_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n","\n","# Joining the words back into a single text\n","df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join(x))"],"metadata":{"id":"JbiTQuMQIWa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = []\n","for text in df['clean_text']:\n","    words.extend(text.split())\n","word_count = collections.Counter(words)\n","top_words = dict(word_count.most_common(10))\n","\n","# Figure Size and set background\n","plt.figure(figsize = (10, 6))\n","plt.style.use('dark_background')\n","\n","# Create the Barplot\n","plt.bar(range(len(top_words)), list(top_words.values()), align = 'center')\n","\n","# Creating a y axis with words\n","plt.xticks(range(len(top_words)), list(top_words.keys()))\n","\n","# Grid Opacity\n","plt.grid(alpha = 0.5)\n","# Title and labels\n","plt.title('Top 10 most used words', fontsize = 18)\n","plt.xlabel('Words')\n","plt.ylabel('Frequency')"],"metadata":{"id":"N0GMMdu6IYeY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Maximum number of words to be considered in the vocabulary\n","max_words = 200\n","# Maximum number of tokens in a sequence\n","max_len = 200\n","# Tokenizer\n","tokenizer = Tokenizer(num_words = max_words)\n","# Snap tokenizer to text data\n","tokenizer.fit_on_texts(df['Text'])\n","# Converts texts into strings of numbers\n","sequences = tokenizer.texts_to_sequences(df['Text'])\n","# Mapping words to indexes\n","word_index = tokenizer.word_index"],"metadata":{"id":"vatgslJZIcHQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sequence padding\n","data = pad_sequences(sequences, maxlen = max_len)"],"metadata":{"id":"e3VW47VKImtQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['Label'] = df['Label'].round().astype(int)"],"metadata":{"id":"TqoGzrDnIqjx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting labels to numeric format\n","labels = tf.keras.utils.to_categorical(df['Label'])"],"metadata":{"id":"mWpfF20pIrlZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Model"],"metadata":{"id":"dog9GvQoI0up"}},{"cell_type":"code","source":["from keras.optimizers import Nadam\n","custom_learning_rate = 0.01\n","optimizer = Nadam(learning_rate=custom_learning_rate)"],"metadata":{"id":"mz1gM2NdItbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras import backend as K"],"metadata":{"id":"TcPEQvppIwT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def specificity(y_true, y_pred):\n","    true_negatives = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.equal(y_true, 0), tf.equal(tf.round(y_pred), 0)), dtype=tf.float32))\n","    possible_negatives = tf.reduce_sum(tf.cast(tf.equal(y_true, 0), dtype=tf.float32))\n","    return true_negatives / (possible_negatives + tf.keras.backend.epsilon())\n","\n","def sensitivity(y_true, y_pred):\n","    true_positives = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.equal(y_true, 1), tf.equal(tf.round(y_pred), 1)), dtype=tf.float32))\n","    possible_positives = tf.reduce_sum(tf.cast(tf.equal(y_true, 1), dtype=tf.float32))\n","    return true_positives / (possible_positives + tf.keras.backend.epsilon())\n","def recall_m(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","def precisionM(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1_S(y_true, y_pred):\n","    precision = precisionM(y_true, y_pred)\n","    recall = recall_m(y_true, y_pred)\n","    return 2*((precision*recall)/(precision+recall+K.epsilon()))"],"metadata":{"id":"3_avYU1bIxwQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model\n","model = Sequential()\n","model.add(Embedding(max_words, 128, input_length = max_len))\n","model.add(Bidirectional(LSTM(64, dropout = 0.3, recurrent_dropout = 0.2)))\n","model.add(Dense(32, activation = 'relu'))\n","model.add(Dense(2, activation = 'softmax'))\n","\n","# Compile the model\n","model.compile(loss = 'categorical_crossentropy', optimizer = optimizer, metrics = ['accuracy',sensitivity, specificity,f1_S])"],"metadata":{"id":"JzCYkgAGI3tI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking summary\n","model.summary()"],"metadata":{"id":"RkllBv6TI58A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fit model_Learning\n","history = model.fit(data, labels, validation_split = 0.3, epochs = 5, batch_size = 4)"],"metadata":{"id":"D_5d775iI8cQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#ACCURACY GRAPH\n","plt.style.use('seaborn')\n","plt.plot(history.history['accuracy'], label='Training Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"-I-HM1hcJBWI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Specificity\n","plt.style.use('seaborn')\n","plt.plot(history.history['specificity'], label='Training Specificity')\n","plt.plot(history.history['val_specificity'], label='Validation Specificity')\n","plt.title('Model Specificity')\n","plt.xlabel('Epoch')\n","plt.ylabel('Specificity')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"5Ft8iXt5JB4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Sensitivity\n","plt.style.use('seaborn')\n","plt.plot(history.history['sensitivity'], label='Training Sensitivity')\n","plt.plot(history.history['val_sensitivity'], label='Validation Sensitivity')\n","plt.title('Model Sensitivity')\n","plt.xlabel('Epoch')\n","plt.ylabel('Sensitivity')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"jaYu8ZSnJDZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#F1_Score\n","plt.style.use('seaborn')\n","plt.plot(history.history['f1_S'], label='Training f1_S')\n","plt.plot(history.history['val_f1_S'], label='Validation f1_S')\n","plt.title('Model F1_Score')\n","plt.xlabel('Epoch')\n","plt.ylabel('f1_S')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"NLnbhb_XJFFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Loss\n","plt.style.use('seaborn')\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Training', 'Validation'], loc = 'upper right')\n","plt.show()"],"metadata":{"id":"EtEC5VCRJG7A"},"execution_count":null,"outputs":[]}]}